<!doctype html><html lang=zh-tw><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="noodp"><title>(RAG技術指南-3)自然語言處理的關鍵技術：從詞嵌入到 Transformer - Allen's blog</title><meta name=Description content="分享 AI、技術、電影的小天地"><meta property="og:url" content="https://www.allenverse.tech/word_embedding/">
<meta property="og:site_name" content="Allen's blog"><meta property="og:title" content="(RAG技術指南-3)自然語言處理的關鍵技術：從詞嵌入到 Transformer"><meta property="og:description" content="文章深入介紹自然語言處理核心技術，從基本詞嵌入到革命性的Transformer模型，展示如何讓電腦理解人類語言的語義關係。"><meta property="og:locale" content="zh_tw"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-04-10T16:02:50+08:00"><meta property="article:modified_time" content="2025-04-10T16:37:24+08:00"><meta property="article:tag" content="LLM"><meta property="article:tag" content="RAG技術指南"><meta property="og:image" content="https://i.imgur.com/9ea7Ahi.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://i.imgur.com/9ea7Ahi.png"><meta name=twitter:title content="(RAG技術指南-3)自然語言處理的關鍵技術：從詞嵌入到 Transformer"><meta name=twitter:description content="文章深入介紹自然語言處理核心技術，從基本詞嵌入到革命性的Transformer模型，展示如何讓電腦理解人類語言的語義關係。"><meta name=application-name content="Allen's blog"><meta name=apple-mobile-web-app-title content="Allen's blog"><meta name=theme-color content="#ffffff"><meta name=msapplication-TileColor content="#da532c"><link rel=icon href=https://i.imgur.com/9ea7Ahi.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=manifest href=/site.webmanifest><link rel=canonical href=https://www.allenverse.tech/word_embedding/><link rel=prev href=https://www.allenverse.tech/rag_concept/><link rel=stylesheet href=/css/style.min.css><link rel=preload href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.7.2/css/all.min.css as=style onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.7.2/css/all.min.css></noscript><link rel=preload href=https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css as=style onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css></noscript><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"(RAG技術指南-3)自然語言處理的關鍵技術：從詞嵌入到 Transformer","inLanguage":"zh-tw","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/www.allenverse.tech\/word_embedding\/"},"image":["https:\/\/i.imgur.com\/9ea7Ahi.png"],"genre":"posts","keywords":"LLM, RAG技術指南","wordcount":1471,"url":"https:\/\/www.allenverse.tech\/word_embedding\/","datePublished":"2025-04-10T16:02:50+08:00","dateModified":"2025-04-10T16:37:24+08:00","publisher":{"@type":"Organization","name":"Che-Wei Chang","logo":"https:\/\/i.imgur.com\/9ea7Ahi.png"},"author":{"@type":"Person","name":"Che-Wei Chang"},"description":""}</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/katex.min.css integrity=sha384-zh0CIslj+VczCZtlzBcjt5ppRcsAmDnRem7ESsYwWwg3m/OaJ2l4x7YBZl9Kxxib crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/katex.min.js integrity=sha384-Rma6DA2IPUwhNxmrB/7S3Tno0YY7sFu9WSYMCuulLhIqYSGZ2gKCJWIqhBWqMQfh crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/contrib/auto-render.min.js integrity=sha384-hCXGrW6PitJEwbkoStFjeJxv+fSOOQKOPbJxSfM6G5sWZjAyWhXiTIIAmQqnlLlh crossorigin=anonymous onload=renderMathInElement(document.body)></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"\\[",right:"\\]",display:!0},{left:"$$",right:"$$",display:!0},{left:"\\(",right:"\\)",display:!1}],throwOnError:!1})})</script></head><body data-header-desktop=fixed data-header-mobile=auto><script>(window.localStorage&&localStorage.getItem("theme")?localStorage.getItem("theme")==="dark":"auto"==="auto"?window.matchMedia("(prefers-color-scheme: dark)").matches:"auto"==="dark")&&document.body.setAttribute("theme","dark")</script><div id=mask></div><div class=wrapper><header class=desktop id=header-desktop><div class=header-wrapper><div class=header-title><a href=/ title="Allen's blog"><img class="lazyload logo" src=/svg/loading.min.svg data-src=https://i.imgur.com/9ea7Ahi.png data-srcset="https://i.imgur.com/9ea7Ahi.png, https://i.imgur.com/9ea7Ahi.png 1.5x, https://i.imgur.com/9ea7Ahi.png 2x" data-sizes=auto alt=https://i.imgur.com/9ea7Ahi.png title=https://i.imgur.com/9ea7Ahi.png>Allen's blog</a></div><div class=menu><div class=menu-inner><a class=menu-item href=/posts/>Posts </a><a class=menu-item href=/tags/>Tags </a><a class=menu-item href=/categories/>Categories </a><a class=menu-item href=/about/ title=About>About </a><span class="menu-item delimiter"></span><a href=javascript:void(0); class="menu-item theme-switch" title=切換主題>
<i class="fas fa-adjust fa-fw" aria-hidden=true></i></a></div></div></div></header><header class=mobile id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title="Allen's blog"><img class="lazyload logo" src=/svg/loading.min.svg data-src=https://i.imgur.com/9ea7Ahi.png data-srcset="https://i.imgur.com/9ea7Ahi.png, https://i.imgur.com/9ea7Ahi.png 1.5x, https://i.imgur.com/9ea7Ahi.png 2x" data-sizes=auto alt=https://i.imgur.com/9ea7Ahi.png title=https://i.imgur.com/9ea7Ahi.png>Allen's blog</a></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><div class=menu id=menu-mobile><a class=menu-item href=/posts/ title>Posts</a><a class=menu-item href=/tags/ title>Tags</a><a class=menu-item href=/categories/ title>Categories</a><a class=menu-item href=/about/ title=About>About</a><a href=javascript:void(0); class="menu-item theme-switch" title=切換主題>
<i class="fas fa-adjust fa-fw" aria-hidden=true></i></a></div></div></header><main class=main><div class=container><div class=toc id=toc-auto><h2 class=toc-title>目錄</h2><div class=toc-content id=toc-content-auto></div></div><article class="page single"><h1 class="single-title animate__animated animate__flipInX">(RAG技術指南-3)自然語言處理的關鍵技術：從詞嵌入到 Transformer</h1><div class=post-meta><div class=post-meta-line><span class=post-author><a href=/ title=Author rel=author class=author><i class="fas fa-user-circle fa-fw" aria-hidden=true></i>Che-Wei Chang</a></span>&nbsp;<span class=post-category>收錄於 <a href=/categories/llm/><i class="far fa-folder fa-fw" aria-hidden=true></i>LLM</a>&nbsp;<a href=/categories/rag%E6%8A%80%E8%A1%93%E6%8C%87%E5%8D%97/><i class="far fa-folder fa-fw" aria-hidden=true></i>RAG技術指南</a></span></div><div class=post-meta-line><i class="far fa-calendar-alt fa-fw" aria-hidden=true></i>&nbsp;<time datetime=2025-04-10>2025-04-10</time>&nbsp;<i class="fas fa-pencil-alt fa-fw" aria-hidden=true></i>&nbsp;約 1471 字&nbsp;
<i class="far fa-clock fa-fw" aria-hidden=true></i>&nbsp;預計閱讀 3 分鐘&nbsp;</div></div><div class=featured-image><img class=lazyload src=/svg/loading.min.svg data-src=https://imgur.com/RafkQ8I.png data-srcset="https://imgur.com/RafkQ8I.png, https://imgur.com/RafkQ8I.png 1.5x, https://imgur.com/RafkQ8I.png 2x" data-sizes=auto alt=https://imgur.com/RafkQ8I.png title=https://imgur.com/RafkQ8I.png></div><div class="details toc" id=toc-static data-kept><div class="details-summary toc-title"><span>目錄</span>
<span><i class="details-icon fas fa-angle-right" aria-hidden=true></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#詞嵌入讓電腦理解人類語言的第一步>詞嵌入：讓電腦理解人類語言的第一步</a><ul><li><a href=#數字背後的意義>數字背後的意義</a></li><li><a href=#相似度的計算方法>相似度的計算方法</a></li><li><a href=#高維詞向量的世界>高維詞向量的世界</a></li></ul></li><li><a href=#transformer-模型突破詞嵌入的限制>Transformer 模型：突破詞嵌入的限制</a><ul><li><a href=#為什麼需要-transformer>為什麼需要 Transformer？</a></li><li><a href=#attention-機制的魔力>Attention 機制的魔力</a></li></ul></li><li><a href=#實際應用簡單的相似度計算示例>實際應用：簡單的相似度計算示例</a></li></ul></nav></div></div><div class=content id=content><h1 id=自然語言處理的關鍵技術從詞嵌入到-transformer>自然語言處理的關鍵技術：從詞嵌入到 Transformer</h1><h2 id=詞嵌入讓電腦理解人類語言的第一步>詞嵌入：讓電腦理解人類語言的第一步</h2><p>電腦只懂得處理二進制訊號（0和1），本質上無法直接理解人類的語言與文字。那麼，我們如何讓機器理解文字的含義呢？答案是透過<strong>數值表示法</strong>。</p><p><img class=lazyload src=/svg/loading.min.svg data-src=https://imgur.com/WzBYrof.png data-srcset="https://imgur.com/WzBYrof.png, https://imgur.com/WzBYrof.png 1.5x, https://imgur.com/WzBYrof.png 2x" data-sizes=auto alt=https://imgur.com/WzBYrof.png title=文字轉換為數字示意圖></p><p>如上圖所示，我們通過編碼器（Encoder）將文字轉換成一系列數字，這些數字就是電腦能夠處理的信息。</p><h3 id=數字背後的意義>數字背後的意義</h3><p>這些數字究竟代表什麼？它們最重要的功能是<strong>計算相似度</strong>，讓電腦能夠通過數學函式理解文字中隱含的意義。</p><p><img class=lazyload src=/svg/loading.min.svg data-src=https://imgur.com/QqMGdV7.png data-srcset="https://imgur.com/QqMGdV7.png, https://imgur.com/QqMGdV7.png 1.5x, https://imgur.com/QqMGdV7.png 2x" data-sizes=auto alt=https://imgur.com/QqMGdV7.png title=相似度示意圖></p><p>以上圖為例，從人類角度來看，「我愛喝拿鐵」和「美式咖啡超讚」這兩句話雖然不完全相同，但確實存在一定的相似度：</p><ul><li>「我愛喝」隱含著讚美的意思</li><li>「拿鐵」和「美式咖啡」都是相近的飲品類別</li></ul><p>理想情況下，當我們將這兩句話通過編碼器轉換成數字向量（稱為「詞嵌入」或「Embedding」）後，這兩組數字應該彼此接近。</p><h3 id=相似度的計算方法>相似度的計算方法</h3><p>那麼，如何計算這種相似度呢？</p><p>首先，我們可以使用直線距離公式：</p>$$ 距離 = sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2} $$<p>距離越小，表示兩個詞嵌入越相似，意味著語義越接近。但這種方法存在一個問題：距離是無界的，難以判斷「300」這樣的距離值究竟意味著相近還是相遠。</p><p>因此，我們採用一種範圍在 <code>-1~1</code> 之間的度量方式：<strong>餘弦相似度</strong>（Cosine Similarity）：</p>$$ Cosine \ Similarity = \frac{x_1 \cdot x_2 + y_1 \cdot y_2}{\sqrt{x_1^2 + y_1^2} \cdot \sqrt{x_2^2 + y_2^2}} $$<p>這個公式的結果範圍是：</p><ul><li>1：兩個向量完全相同（方向一致）</li><li>0：兩個向量正交（不相關）</li><li>-1：兩個向量方向相反</li></ul><p><img class=lazyload src=/svg/loading.min.svg data-src=https://imgur.com/9c3ISa6.png data-srcset="https://imgur.com/9c3ISa6.png, https://imgur.com/9c3ISa6.png 1.5x, https://imgur.com/9c3ISa6.png 2x" data-sizes=auto alt=https://imgur.com/9c3ISa6.png title=詞向量示意圖></p><p>從上圖可以看出，「balls」和「kick」這兩個詞的餘弦夾角很小，表示它們在語義空間中非常接近。同理，我們期望「我愛喝拿鐵」和「美式咖啡超讚」的詞嵌入也應該被映射到相近的位置。</p><h3 id=高維詞向量的世界>高維詞向量的世界</h3><p>實際應用中，兩個維度遠遠不足以表達語言的複雜性。現代詞嵌入模型（如BERT）通常使用數百維的向量（如512維）來表示文字。</p><p><img class=lazyload src=/svg/loading.min.svg data-src=https://imgur.com/QXiEnth.png data-srcset="https://imgur.com/QXiEnth.png, https://imgur.com/QXiEnth.png 1.5x, https://imgur.com/QXiEnth.png 2x" data-sizes=auto alt=https://imgur.com/QXiEnth.png title=高維詞向量示意圖></p><p>詞嵌入模型的訓練目標，就是學習如何將輸入的文字準確映射到這個高維空間的適當位置，使得語義相似的詞或句子在空間中彼此接近。</p><h2 id=transformer-模型突破詞嵌入的限制>Transformer 模型：突破詞嵌入的限制</h2><h3 id=為什麼需要-transformer>為什麼需要 Transformer？</h3><p>傳統詞嵌入方法存在一個關鍵問題：無法處理一詞多義的情況。考慮以下兩個例子：</p><ul><li>「我愛吃蘋果」</li><li>「iPhone16好好用，我好愛蘋果」</li></ul><p>在傳統詞嵌入模型中，這兩個句子中的「蘋果」會被賦予相同的向量表示，因為模型無法識別上下文。結果，無論「蘋果」指的是水果還是科技公司，它們都被映射到同一個位置。</p><p><img class=lazyload src=/svg/loading.min.svg data-src=https://imgur.com/0g046Ot.png data-srcset="https://imgur.com/0g046Ot.png, https://imgur.com/0g046Ot.png 1.5x, https://imgur.com/0g046Ot.png 2x" data-sizes=auto alt=https://imgur.com/0g046Ot.png title=一詞多義問題示意圖></p><p>如上圖所示，「蘋果手機」中的「蘋果」本應與「手機」更接近，但在傳統模型中卻與食物的「蘋果」更相似。</p><p>解決方案？—— <strong>Attention is all you need!</strong></p><h3 id=attention-機制的魔力>Attention 機制的魔力</h3><p>Transformer 模型的核心是注意力（Attention）機制，它能夠根據上下文動態調整詞的表示。</p><p><img class=lazyload src=/svg/loading.min.svg data-src=https://imgur.com/egEQSgs.png data-srcset="https://imgur.com/egEQSgs.png, https://imgur.com/egEQSgs.png 1.5x, https://imgur.com/egEQSgs.png 2x" data-sizes=auto alt=https://imgur.com/egEQSgs.png title=Attention機制示意圖></p><p>通過 Attention 機制，「蘋果手機」中的「蘋果」現在與「手機」更接近，而與食物的「蘋果」更遠。這正是我們期望的結果！</p><p>在實際應用中，每種語言都有許多複雜的語義現象，這就是為什麼我們選擇基於 Attention 機制的 Transformer 架構作為編碼器的基礎。在後續討論中，我們將把這類編碼器統稱為「密集檢索器」（Dense Retriever）。</p><h2 id=實際應用簡單的相似度計算示例>實際應用：簡單的相似度計算示例</h2><p>以下是一個簡單的程式碼示例，展示如何計算句子之間的相似度：</p><div class="code-block code-line-numbers" style="counter-reset:code-block 0"><div class="code-header language-python"><span class=code-title><i class="arrow fas fa-chevron-right fa-fw" aria-hidden=true></i></span>
<span class=ellipses><i class="fas fa-ellipsis-h fa-fw" aria-hidden=true></i></span>
<span class=copy title=複製到剪貼板><i class="far fa-copy fa-fw" aria-hidden=true></i></span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 計算相似度.py</span>
</span></span><span class=line><span class=cl><span class=n>sentenceA</span> <span class=o>=</span> <span class=s2>&#34;我愛蘋果手機&#34;</span>
</span></span><span class=line><span class=cl><span class=n>sentenceB</span> <span class=o>=</span> <span class=s2>&#34;我愛吃蘋果&#34;</span>
</span></span><span class=line><span class=cl><span class=n>sentenceC</span> <span class=o>=</span> <span class=s2>&#34;我覺得 iphone 很好用&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>embeddingA</span> <span class=o>=</span> <span class=n>Encoder</span><span class=o>.</span><span class=n>encode</span><span class=p>(</span><span class=n>sentenceA</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>embeddingB</span> <span class=o>=</span> <span class=n>Encoder</span><span class=o>.</span><span class=n>encode</span><span class=p>(</span><span class=n>sentenceB</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>embeddingC</span> <span class=o>=</span> <span class=n>Encoder</span><span class=o>.</span><span class=n>encode</span><span class=p>(</span><span class=n>sentenceC</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 使用餘弦相似度計算</span>
</span></span><span class=line><span class=cl><span class=n>ab_similarity</span> <span class=o>=</span> <span class=n>cosine_similarity</span><span class=p>(</span><span class=n>embeddingA</span><span class=p>,</span> <span class=n>embeddingB</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>ac_similarity</span> <span class=o>=</span> <span class=n>cosine_similarity</span><span class=p>(</span><span class=n>embeddingA</span><span class=p>,</span> <span class=n>embeddingC</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=n>ab_similarity</span> <span class=o>&gt;</span> <span class=n>ac_similarity</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;</span><span class=si>{</span><span class=n>sentenceA</span><span class=si>}</span><span class=s2> 跟 </span><span class=si>{</span><span class=n>sentenceB</span><span class=si>}</span><span class=s2> 比較像&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;</span><span class=si>{</span><span class=n>sentenceA</span><span class=si>}</span><span class=s2> 跟 </span><span class=si>{</span><span class=n>sentenceC</span><span class=si>}</span><span class=s2> 比較像&#34;</span><span class=p>)</span></span></span></code></pre></div></div><p>根據所選的編碼器不同（如 Word2Vec 或 BERT），結果也會有所不同。使用考慮上下文的 Transformer 模型（如 BERT）時，我們可以期待得到更符合人類直覺的相似度計算結果。</p></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span>更新於 2025-04-10&nbsp;<a class=git-hash href=https://github.com/dillonzq/LoveIt/commit/041e3cdb54418147d40c3dbc389a2a1a62cbf9d7 target=_blank title="commit by allen3325(00857146@email.ntou.edu.tw) 041e3cdb54418147d40c3dbc389a2a1a62cbf9d7: Update: 2025-04-10 16:37">
<i class="fas fa-hashtag fa-fw" aria-hidden=true></i>041e3cd</a></span></div></div><div class=post-info-line><div class=post-info-md><span><a class=link-to-markdown href=/word_embedding/index.md target=_blank>閱讀原始文檔</a></span></div><div class=post-info-share><span><a href=javascript:void(0); title="分享到 Facebook" data-sharer=facebook data-url=https://www.allenverse.tech/word_embedding/ data-hashtag=LLM><i class="fab fa-facebook-square fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="分享到 Linkedin" data-sharer=linkedin data-url=https://www.allenverse.tech/word_embedding/><i class="fab fa-linkedin fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="分享到 Line" data-sharer=line data-url=https://www.allenverse.tech/word_embedding/ data-title="(RAG技術指南-3)自然語言處理的關鍵技術：從詞嵌入到 Transformer"><i data-svg-src=https://cdn.jsdelivr.net/npm/simple-icons@14.9.0/icons/line.svg aria-hidden=true></i></a><a href="https://t.me/share/url?url=https%3a%2f%2fwww.allenverse.tech%2fword_embedding%2f&amp;text=%28RAG%e6%8a%80%e8%a1%93%e6%8c%87%e5%8d%97-3%29%e8%87%aa%e7%84%b6%e8%aa%9e%e8%a8%80%e8%99%95%e7%90%86%e7%9a%84%e9%97%9c%e9%8d%b5%e6%8a%80%e8%a1%93%ef%bc%9a%e5%be%9e%e8%a9%9e%e5%b5%8c%e5%85%a5%e5%88%b0%20Transformer" target=_blank title="分享到 Telegram"><i class="fab fa-telegram fa-fw" aria-hidden=true></i></a></span></div></div></div><div class=post-info-more><section class=post-tags><i class="fas fa-tags fa-fw" aria-hidden=true></i>&nbsp;<a href=/tags/llm/>LLM</a>,&nbsp;<a href=/tags/rag%E6%8A%80%E8%A1%93%E6%8C%87%E5%8D%97/>RAG技術指南</a></section><section><span><a href=javascript:void(0); onclick=window.history.back()>返回</a></span>&nbsp;|&nbsp;<span><a href=/>主頁</a></span></section></div><div class=post-nav><a href=/rag_concept/ class=prev rel=prev title="(RAG技術指南-2)RAG 技術：為 AI 模型增添參考資料的能力"><i class="fas fa-angle-left fa-fw" aria-hidden=true></i>(RAG技術指南-2)RAG 技術：為 AI 模型增添參考資料的能力</a></div></div><div id=comments></div></article></div></main><footer class=footer><div class=footer-container><div class=footer-line>由 <a href=https://gohugo.io/ target=_blank rel="noopener noreffer" title="Hugo 0.145.0">Hugo</a> 強力驅動 | 主題 - <a href=https://github.com/dillonzq/LoveIt target=_blank rel="noopener noreffer" title="LoveIt 0.3.0"><i class="far fa-kiss-wink-heart fa-fw" aria-hidden=true></i> LoveIt</a></div><div class=footer-line itemscope itemtype=http://schema.org/CreativeWork><i class="far fa-copyright fa-fw" aria-hidden=true></i><span itemprop=copyrightYear>2025</span><span class=author itemprop=copyrightHolder>&nbsp;<a href=/ target=_blank>Che-Wei Chang</a></span></div></div></footer></div><div id=fixed-buttons><a href=# id=back-to-top class=fixed-button title=回到頂部><i class="fas fa-arrow-up fa-fw" aria-hidden=true></i></a></div><div id=fixed-buttons-hidden><a href=# id=view-comments class=fixed-button title=查看評論><i class="fas fa-comment fa-fw" aria-hidden=true></i></a></div><script src=https://cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js></script><script src=https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js></script><script src=https://cdn.jsdelivr.net/npm/sharer.js@0.5.2/sharer.min.js></script><script>window.config={comment:{}}</script><script src=/js/theme.min.js></script></body></html>